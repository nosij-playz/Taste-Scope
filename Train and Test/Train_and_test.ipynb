{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset\n"
      ],
      "metadata": {
        "id": "36EGIN90tqVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dansbecker/food-101\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQleiDMFQfL",
        "outputId": "de34bb25-c3c9-4d83-e10b-375b943a885b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/food-101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arrangement of Dataset"
      ],
      "metadata": {
        "id": "ckV0luvMttKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_path = \"/kaggle/input/food-101\"\n",
        "target_path = \"datasets/food-101\"\n",
        "\n",
        "# Create target folder if not exist\n",
        "os.makedirs(target_path, exist_ok=True)\n",
        "\n",
        "# Copy all files and folders from source to target\n",
        "print(f\"Copying data from {source_path} to {target_path}...\")\n",
        "\n",
        "for item in os.listdir(source_path):\n",
        "    s = os.path.join(source_path, item)\n",
        "    d = os.path.join(target_path, item)\n",
        "    if os.path.isdir(s):\n",
        "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
        "    else:\n",
        "        shutil.copy2(s, d)\n",
        "\n",
        "print(\"Copy complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxszptSNFTIp",
        "outputId": "61d4fc11-039c-4538-a012-4933343f3e7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data from /kaggle/input/food-101 to datasets/food-101...\n",
            "Copy complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "qsqQbJk8tybd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "4HROn8SFsKww"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "files path parameters predefine"
      ],
      "metadata": {
        "id": "mhjEEL3btz09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable cuDNN auto-tuner for better performance on fixed-size input\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/datasets/food-101/food-101/food-101/images\"\n",
        "TRAIN_META = \"/content/datasets/food-101/food-101/food-101/meta/train.txt\"\n",
        "TEST_META = \"/content/datasets/food-101/food-101/food-101/meta/test.txt\"\n",
        "SPLIT_DIR = \"datasets/food-101_split\"\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 101"
      ],
      "metadata": {
        "id": "CJLMt_22sQcH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoswitch according device"
      ],
      "metadata": {
        "id": "dccPKR31t7k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "cEIMkCo-sSPX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test train split"
      ],
      "metadata": {
        "id": "xxZgEyC-uCCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split folders creation\n",
        "def create_split_folders(root_dir, train_file, test_file, output_dir):\n",
        "    train_output = os.path.join(output_dir, \"train\")\n",
        "    val_output = os.path.join(output_dir, \"val\")\n",
        "\n",
        "    os.makedirs(train_output, exist_ok=True)\n",
        "    os.makedirs(val_output, exist_ok=True)\n",
        "\n",
        "    def copy_files(file_list_path, dest_root):\n",
        "        with open(file_list_path, 'r') as f:\n",
        "            for line in f:\n",
        "                class_name, img_file = line.strip().split('/')\n",
        "                src_path = os.path.join(root_dir, class_name, img_file + \".jpg\")\n",
        "                class_folder = os.path.join(dest_root, class_name)\n",
        "                os.makedirs(class_folder, exist_ok=True)\n",
        "                dst_path = os.path.join(class_folder, img_file + \".jpg\")\n",
        "                if not os.path.exists(dst_path):\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    copy_files(train_file, train_output)\n",
        "    copy_files(test_file, val_output)\n",
        "    print(f\"Created training folder at: {train_output}\")\n",
        "    print(f\"Created validation folder at: {val_output}\")"
      ],
      "metadata": {
        "id": "au_RzNumsUo_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Hold On step"
      ],
      "metadata": {
        "id": "NEvJE_s4uLGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run once\n",
        "create_split_folders(DATA_DIR, TRAIN_META, TEST_META, SPLIT_DIR)"
      ],
      "metadata": {
        "id": "C2lcLtnDsXtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af3be0e-200b-44c9-cd91-bdbbf5dcf9d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created training folder at: datasets/food-101_split/train\n",
            "Created validation folder at: datasets/food-101_split/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading splitted datasets"
      ],
      "metadata": {
        "id": "THD3vldRuOj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = os.cpu_count()\n",
        "train_dataset = datasets.ImageFolder(os.path.join(SPLIT_DIR, \"train\"), transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(SPLIT_DIR, \"val\"), transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "pssmHgdqsZ1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64e8a33-5c16-464f-8095-6afef6ed135a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 75750\n",
            "Number of validation samples: 25250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using RESNET50"
      ],
      "metadata": {
        "id": "trU3Z-lOuS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model setup\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "H-G2LjRWsdto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866a244b-2ab3-4922-a11f-aaab0c112fac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 133MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Function with split validation"
      ],
      "metadata": {
        "id": "O5n1GvgcuVmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_food101_resnet50.pth\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(f\"Best validation accuracy: {best_acc:.4f}\")"
      ],
      "metadata": {
        "id": "jkVXnu6Wsi8P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering"
      ],
      "metadata": {
        "id": "VH5jsprKubQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extractor class\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # remove fc layer\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.features(x)\n",
        "            x = self.flatten(x)  # [B, 2048]\n",
        "        return x\n",
        "\n",
        "# Extract features from dataloader\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            features = model(inputs)\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    features_tensor = torch.cat(all_features)\n",
        "    labels_tensor = torch.cat(all_labels)\n",
        "    return features_tensor, labels_tensor\n"
      ],
      "metadata": {
        "id": "pd5Viq2Asn9A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training model using extracted features"
      ],
      "metadata": {
        "id": "p2xlhDbfuieH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)\n",
        "\n",
        "# Feature extraction after training\n",
        "feature_model = FeatureExtractor(models.resnet50(pretrained=True).to(device))\n",
        "train_feats, train_lbls = extract_features(feature_model, train_loader)\n",
        "val_feats, val_lbls = extract_features(feature_model, val_loader)\n",
        "\n",
        "torch.save((train_feats, train_lbls), \"train_features.pt\")\n",
        "torch.save((val_feats, val_lbls), \"val_features.pt\")\n",
        "print(\"Extracted and saved feature embeddings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwL107BjFZ44",
        "outputId": "b57cb696-7b59-4d77-b200-7ea034a622e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 2.0729 | Train Acc: 0.4980 | Val Loss: 1.0413 | Val Acc: 0.7179 | Time: 857.3s\n",
            "Epoch 2/10 | Train Loss: 1.4989 | Train Acc: 0.6203 | Val Loss: 0.8457 | Val Acc: 0.7653 | Time: 852.1s\n",
            "Epoch 3/10 | Train Loss: 1.3403 | Train Acc: 0.6584 | Val Loss: 0.7847 | Val Acc: 0.7824 | Time: 850.4s\n",
            "Epoch 4/10 | Train Loss: 1.2386 | Train Acc: 0.6816 | Val Loss: 0.7152 | Val Acc: 0.8019 | Time: 852.8s\n",
            "Epoch 5/10 | Train Loss: 1.1737 | Train Acc: 0.6944 | Val Loss: 0.6895 | Val Acc: 0.8101 | Time: 851.3s\n",
            "Epoch 6/10 | Train Loss: 1.1046 | Train Acc: 0.7123 | Val Loss: 0.6703 | Val Acc: 0.8160 | Time: 852.5s\n",
            "Epoch 7/10 | Train Loss: 1.0565 | Train Acc: 0.7232 | Val Loss: 0.6502 | Val Acc: 0.8222 | Time: 852.7s\n",
            "Epoch 8/10 | Train Loss: 1.0235 | Train Acc: 0.7327 | Val Loss: 0.6274 | Val Acc: 0.8290 | Time: 852.7s\n",
            "Epoch 9/10 | Train Loss: 0.9848 | Train Acc: 0.7414 | Val Loss: 0.6433 | Val Acc: 0.8258 | Time: 847.6s\n",
            "Epoch 10/10 | Train Loss: 0.9404 | Train Acc: 0.7513 | Val Loss: 0.6558 | Val Acc: 0.8214 | Time: 848.2s\n",
            "Best validation accuracy: 0.8290\n",
            "Extracted and saved feature embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on external and unseen data"
      ],
      "metadata": {
        "id": "_o-prXSmumf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "ow-LizK3tOpo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "MODEL_PATH = \"best_food101_resnet50.pth\"\n",
        "NUM_CLASSES = 101\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "BvCsgvPJtTqu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transform (must match validation transforms)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "G0LwwbI-tVMF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "def load_model(model_path):\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "wCfIk0N2tWf2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict function\n",
        "def predict(image_path, model, class_names):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        return class_names[predicted.item()]"
      ],
      "metadata": {
        "id": "uPEKsQuZtZYG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class names (from folder structure)\n",
        "def load_class_names(train_dir):\n",
        "    return sorted(os.listdir(train_dir))"
      ],
      "metadata": {
        "id": "yxatKDMetav9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = load_model(MODEL_PATH)\n",
        "class_names = load_class_names(\"datasets/food-101_split/train\")\n",
        "\n",
        "test_image_path = \"/content/datasets/food-101_split/train/beet_salad/1003501.jpg\"\n",
        "prediction = predict(test_image_path, model, class_names)\n",
        "print(f\"Predicted class: {prediction}\")"
      ],
      "metadata": {
        "id": "DXw-nPj-Fh9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfeae12a-a11e-4721-d7e3-47d92f82466d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: beet_salad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating classes(classify)"
      ],
      "metadata": {
        "id": "m9C-9Fw2WZJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to training dataset\n",
        "TRAIN_DIR = \"datasets/food-101_split/train\"\n",
        "OUTPUT_JSON = \"class_names.json\"\n",
        "\n",
        "def save_class_names(train_dir, output_path):\n",
        "    # Get sorted list of class names\n",
        "    class_names = sorted(os.listdir(train_dir))\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(class_names, f, indent=2)\n",
        "\n",
        "    print(f\"Saved {len(class_names)} class names to {output_path}\")\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    save_class_names(TRAIN_DIR, OUTPUT_JSON)\n"
      ],
      "metadata": {
        "id": "Vn-tSFl2Fj9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5308ca42-7dad-4cbf-8bef-341992b09dc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 101 class names to class_names.json\n"
          ]
        }
      ]
    }
  ]
}